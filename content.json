{"meta":{"title":"Data Miner","subtitle":"As far as the laws of mathematics refer to reality, they are not certain, as far as they are certain, they do not refer to reality. - Albert Einstein","description":null,"author":"mtpgm","url":"http://mtpgm.com"},"pages":[],"posts":[{"title":"从零开始的序章","slug":"newbeginning","date":"2017-01-04T22:02:31.000Z","updated":"2017-01-04T22:16:26.337Z","comments":true,"path":"2017/01/05/newbeginning/","link":"","permalink":"http://mtpgm.com/2017/01/05/newbeginning/","excerpt":"","text":"自感这几年来全浪费了，从来没有认真做好一件事，总是看到有趣的就看看，然后忘记。在2017年开始的第一月，认真读几本书吧。 Theoretical neuroscience: computational and mathematical modeling of neural systems. Deep learning. Probabilistic graphical models. How to build a brain: a neural architecture for biological cognition.","categories":[],"tags":[]},{"title":"静态fMRI数据中的信息流动","slug":"activityflow","date":"2016-12-27T01:36:25.000Z","updated":"2017-01-05T13:38:38.551Z","comments":true,"path":"2016/12/27/activityflow/","link":"","permalink":"http://mtpgm.com/2016/12/27/activityflow/","excerpt":"大脑信号传播是通过突触完成的，具体表现是受到激发的神经元发出一个个spike，激活突触中的信号通路，这个过程一般在微秒级的时间尺度上完成的。通过测量血液中氧气浓度(BOLD信号)来间接反应大脑信号强度的fMRI技术的缺点正是时间分辨率太低，导致不能采集出神经元放电前后的变化，也就不能让我们检测到信号传导的前后顺序。即我们只能从磁共振扫描仪采集的大脑信号看出来大脑每个位置的信号随时间的波动。最近看nature neuroscience的网站，发现了一篇论文——“Activity flow over resting-state networks shapes cognitive task activations“，单单是名字就很吸引人了，从相对静态的fMRI数据发现信息流动，是一个很神奇的设想。","text":"大脑信号传播是通过突触完成的，具体表现是受到激发的神经元发出一个个spike，激活突触中的信号通路，这个过程一般在微秒级的时间尺度上完成的。通过测量血液中氧气浓度(BOLD信号)来间接反应大脑信号强度的fMRI技术的缺点正是时间分辨率太低，导致不能采集出神经元放电前后的变化，也就不能让我们检测到信号传导的前后顺序。即我们只能从磁共振扫描仪采集的大脑信号看出来大脑每个位置的信号随时间的波动。最近看nature neuroscience的网站，发现了一篇论文——“Activity flow over resting-state networks shapes cognitive task activations“，单单是名字就很吸引人了，从相对静态的fMRI数据发现信息流动，是一个很神奇的设想。论文作者把静息态网络作为大脑中相互交错的道路，大脑信号沿着这些路径在各个脑区之间流动，信号传递效率受到突触权重的影响。对于fmri，每个时间点每个脑区信号就是这种信号流动后的结果，也就是说每个脑区的信号是其他脑区信号传递到这个脑区的总和，可以用下面的模型表示，论文首先使用了模拟的数据来检验模型，虽然用了一个非常简单的模型来模拟数据，但对于初学者而言是一个非常有趣的模型。静息态下自发的大脑活动的起源目前并不清楚，但很多研究是通过结构连接来模拟静息态的功能连接的，所以这篇论文也是先建立了脑区间的结构连接，进而调整连接权重实现可塑性的突触连接。论文模拟了300个单元，代表了300个不同的脑区，然后设置网络密度为15%建立结构连接。脑区间的连接（cross-region connections）G取值范围设为0-5，脑区自连（self-connections，也成为reccurrent connections）Indep取值范围设为0-100。接着让结点间的相互连接形成3个结构单元。对于第一个结构单元，再通过调整突触权重形成2个功能单元。这样就生成了一个大脑的突触连接网络，也就是信号传播的道路。 import numpy as np import matplotlib.pyplot as plt from scipy.integrate import odeint //用于计算积分 numRegions = 300 #Creating network model with random connectivity, 15% density with random synaptic strengths structConnVector=np.random.uniform(0,1,(numRegions,numRegions))&gt;.85 #Add self-connections (important if adding autocorrelation later) np.fill_diagonal(structConnVector,10) #Create modular structural network (3 modules) numModules=3 numRPerModule=int(round(numRegions/numModules)) lastModuleNode=-1 for moduleNum in range(0,numModules): for thisNodeNum in range(lastModuleNode+1,lastModuleNode+numRPerModule+1): #Set this node to connect to 50 random other nodes in module for i in range(1,numRPerModule//2): randNodeInModule=int(np.random.uniform(lastModuleNode+1,lastModuleNode+numRPerModule-1,(1,1))) structConnVector[thisNodeNum,randNodeInModule]=1 lastModuleNode=lastModuleNode+numRPerModule #Adding synaptic weights to existing structural connections (small random synapse strength variation) synapticWeightVector=structConnVector*(1+np.random.standard_normal((numRegions,numRegions))*.001) #Adding synaptic mini-communities (within community 1) synapticWeightVector[0:50,50:100]=synapticWeightVector[0:50,50:100]*0.5 synapticWeightVector[50:100,0:50]=synapticWeightVector[50:100,0:50]*0.5 synapticWeightVector[0:50,0:50]=synapticWeightVector[0:50,0:50]*1.5 synapticWeightVector[50:100,50:100]=synapticWeightVector[50:100,50:100]*1.5 #Implement global coupling parameter G=1 synapticWeightVector=G*synapticWeightVector #Implement local independence parameter Indep=1 plt.imshow(synapticWeightVector) plt.colorbar() plt.savefig('ActflowSim_StructMat.png',dpi=600,transparent=True, bbox_inches='tight') 论文接着使用经典的spiking rate model来生成自发的大脑活动和任务态的大脑活动。$$\\tau_i\\frac{dx_i}{dt}=-x_i+f_i(\\sum _{j=1}^{n}w_{ji}x_j + bias_i)\\ \\ \\ i=[1..n]$$静息态：在每个时间点，每个单元的活性都设为0，然后对所有单元同时进行刺激，让刺激引发的spike通过网络传播。自相关项设为AR(1)，系数设为0.1。$$\\varepsilon_t=0.1*\\varepsilon_{t-1}+normal(0,1)$$任务态：对每一个任务，选出5个连续的单元，在需要进行刺激的时间点处，把刺激强度加到静息态随机刺激强度上，然后让活动沿着网络传播开来。就像在一张餐巾纸上面滴一滴墨水，让其扩散开来。 def sigmoid(x): return 1/(1+np.exp(-x)) def equation(startActivity, t, spontActivity, synapticWeightVector, bias=0): x=startActivity dxdt = -x + sigmoid(np.dot(spontActivity, synapticWeightVector)+bias) return dxdt def networkModel(G=1.0, Indep = 1.0, stimTimes=[], stimRegions=None, synapticWeightVector=synapticWeightVector, numTimePoints=numTimePoints): bias = np.zeros(numRegions) simulatedTimeseries=np.zeros((numTimePoints, numRegions)) #Each state modifies previous state (creating some autocorrelation; pink noise) autocorrFactor=0.10 # Modulate synaptic weight matrix by coupling parameter, G GlobalCouplingMat = synapticWeightVector*G np.fill_diagonal(GlobalCouplingMat,0) # Modulate self connection 'independence' parameter, Indep IndepVarMat = np.identity(numRegions)*Indep IndepVarMat = np.multiply(IndepVarMat,synapticWeightVector) # Now reconstruct synapticWeightMatrix synapticWeightVector = GlobalCouplingMat + IndepVarMat # Begin computing simulation for thisTimePoint in range(0,numTimePoints): #one time step t = np.linspace(0,1,2) #initialize network startActivity = np.zeros(numRegions) # Generate spontaneous activity for initial state spontActVector=np.random.normal(0,1,(numRegions,)) stimActVector=np.zeros(numRegions) # Specify spontaneous input activity at this time point and task activity if thisTimePoint in stimTimes: #Include moment-to-moment variability in task stimulation stimAct=np.ones(len(stimRegions))*np.random.normal(1,0.5,) #stimAct=np.ones(len(stimRegions))*0.5 #excluding moment-to-moment variability in task stimulation stimActVector[stimRegions]=stimActVector[stimRegions]+stimAct # Add spontaneous activity vector with task stimulus spontActVector=(autocorrFactor*spontActVector)+np.random.normal(0,1,(numRegions,))+stimActVector if thisTimePoint==0: # set initial condition simulatedTimeseries[thisTimePoint,] = 0.0 # 0 for all regions else: simulatedTimeseries[i,]+=odeint(equation, startActivity, t, args=(spontActVector,synapticWeightVector,bias))[1,] return simulatedTimeseries simulatedRestTimeseries = networkModel(G=G, Indep=Indep, stimTimes=[], stimRegions=None, numTimePoints=numTimePoints) 与hrf卷积，然后下采样到2s的分辨率 def canonicalHRF(x, param={}): if len(param)!=5: param={'a1':6, 'a2':12, 'b1':0.9, 'b2':0.9, 'c':0.35} d1 = param['a1']*param['b1'] d2 = param['a2']*param['b2'] return ((x/d1)**param['a1']*np.exp(-(x-d1)/param['b1']) - param['c']*(x/d2)**param['a2']*np.exp(-(x-d2)/param['b2'])) simsample_rate=0.1 simsample_times = np.arange(0, 30, simsample_rate) hrf = canonicalHRF(simsample_times) simulatedTimeseries_convolved=np.ones(np.shape(simulatedRestTimeseries)) for regionNum in range(0,numRegions): convolved = np.convolve(simulatedRestTimeseries[:,regionNum], hrf_at_simsample) n_to_remove = len(hrf_at_simsample) - 1 convolved = convolved[:-n_to_remove] simulatedTimeseries_convolved[:,regionNum]=convolved #Downsample fMRI time series TR=2 dt_rec=0.1 n_skip_BOLD = int(TR/dt_rec) BOLD_rec = simulatedTimeseries_convolved[::n_skip_BOLD] #Produce rest FC matrix based on produced spontaneous fMRI time series fcMat_rest=np.corrcoef(BOLD_rec[10:,],rowvar=0) #np.fill_diagonal(fcMat_rest,0) plt.imshow(fcMat_rest) plt.colorbar() plt.savefig('ActflowSim_RestfMRI_FCMat.png',dpi=600,transparent=True, bbox_inches='tight') 剩下的部分非常明了，模拟任务态数据，然后运行activity flow model，接着迭代cross-region connections和self-connection的参数值，从中发现模型所需要的静息态网络的特性。","categories":[],"tags":[{"name":"neuroscience","slug":"neuroscience","permalink":"http://mtpgm.com/tags/neuroscience/"}]},{"title":"C++程序设计","slug":"Cplus-programming","date":"2016-12-18T08:07:56.000Z","updated":"2016-12-30T21:32:16.741Z","comments":true,"path":"2016/12/18/Cplus-programming/","link":"","permalink":"http://mtpgm.com/2016/12/18/Cplus-programming/","excerpt":"函数指针 程序运行期间，每个函数都会占用一段连续的内存空间。函数名就是该函数所占内存空间的起始地址。可以把函数的起始地址赋给一个指针变量，使该指针变量指向该函数，然后通过指针变量就可以调用这个函数。这种指向函数的指针变量称为“函数指针”。 //类型名 （*指针变量名）(参数类型1， 参数类型2， ...); int (*pf)(int, char); //通过函数指针调用它指向的函数 //函数指针名（实参表）;","text":"函数指针 程序运行期间，每个函数都会占用一段连续的内存空间。函数名就是该函数所占内存空间的起始地址。可以把函数的起始地址赋给一个指针变量，使该指针变量指向该函数，然后通过指针变量就可以调用这个函数。这种指向函数的指针变量称为“函数指针”。 //类型名 （*指针变量名）(参数类型1， 参数类型2， ...); int (*pf)(int, char); //通过函数指针调用它指向的函数 //函数指针名（实参表）; 命令行参数 int main(int argc, char *argv[]){ //argc代表程序启动时命令行参数的个数。可执行程序本身的文件名也是一个参数，因此argc的值至少为1. //argv指针数组，其中每个元素都是char *类型的指针，该指针指向一个字符串，这个字符串里存放着命令行参数。例如，argv[0]指向的字符串就是第一个命令行参数，即可执行程序的文件名，argv[1]指向第二个命令行参数，argv[2]指向第三个命令行参数. } 位运算 //按位与“&”，按位或“|“，按位异或“^”，非“~” //左移运算符 &lt;&lt;高位丢弃 低位补零 比乘法快 //右移运算符 &gt;&gt;低位丢弃，对于有符号的int等高位为1就补1，高位为零就补零。 引用 定义引用时一定要初始化成引用某个变量，初始化后它就一直引用该变量，不会再引用别的变量。只能引用变量，不能引用常量和表达式。函数返回值写成引用，非引用的函数返回值不可以作为左值使用。 int n=4; int &r=n; //r引用了n，相当于n的一个别名 const int & t=n; //常引用 不能通过其修改引用的内容 动态内存分配 int * pn = new int; * pn = 5; delete pn; int * pm = new int[10]; delete [] pm; 内联函数 应对函数被反复执行很多次的情况。编译器处理对内联函数的调用语句时，是将整个函数的代码插入到调用语句处，而不会产生调用函数的语句。在函数定义前面加“inline”关键字即可定义内联函数。 inline int Max(int a, int b){ if (a>b) return a; return b; } 函数重载 一个或多个函数，名字相同，返回值类型相同，参数个数或参数类型不同，编译器根据调用语句中的实参个数和类型判断调用哪个函数。不能确定调用哪个函数时出现”二义性“错误。 类 对象的大小等于所有成员变量的大小之和，每个对象各有自己的存储空间。对象之间可以用”=“进行赋值。 缺省为私有成员 class classname{ private: //只能在成员函数内被访问 //私有属性和函数 public: //可以在任何地方被访问 //公有属性和函数 protected: //继承 //保护属性和函数 }; //分号 对象成员的访问权限 类的成员函数内部，可以访问： 当前对象的全部属性，函数 同类其他对象的全部属性和函数 类的成员函数以外的地方， 只能访问该类对象的公有成员。 内联成员函数 inline + 成员函数(声明在类内，定义在类外)整个函数体出现在类定义内部 成员函数重载及参数缺省 构造函数 名字与类名相同，可以有参数，不能有返回值（void也不行），如果定义类时没写构造函数，则编译器生成一个默认的无参数的构造函数，不做任何操作。对象生成时构造函数自动被调用，一个类可以有多个构造函数。 复制构造函数 只有一个参数，即同类对象的引用。如果没有定义复制构造函数，编译器默认生成一个。 X::X( X&) X::X(const X &) //能以常量对象作为参数 类型转换构造函数：只有一个参数，不是复制构造函数，建立一个临时对象。 析构函数 名字与类名相同，在前面加”~“，没有参数和返回值，一个类最多只有一个析构函数 静态成员变量和静态成员函数 在声明前加static，为所有对象共享，不需要通过对象就能访问。sizeof不会计算静态成员变量。静态成员变量本质上是全局变量，哪怕一个对象都不存在，类的静态成员变量也存在。静态成员函数本质上是全局函数，因此函数里面不能访问非静态成员变量，也不能调用非静态成员函数。 必须在定义类的文件中对静态成员变量进行一次初始化。 设置静态成员是为了将和某些类紧密相关的全局变量和函数写到类里面，看上去像一个整体，易于维护和理解。 class CMyclass{ int n; static int s; }; //sizeof(CMyclass)只计算int，所以等于4 int CMyclass::s = 0; //初始化 CMyclass::s; //类名::成员名 CMyclass r; r.s; //对象名.成员名 CMyclass *p=&r; p-&gt;s; //指针-&gt;成员名 CMyclass & ref = r; ref.s; // 引用.成员名 成员对象和封闭类 成员对象：一个类的成员变量是另一个类的对象。 包含成员对象的类叫封闭类（encclosing）。 友元（Friend） 一个类的友元函数可以访问该类的私有成员A是B的友元类，A的成员函数可以访问B的私有成员，友元类之间不能传递，不能继承 this指针 指向成员函数所作用的对象。静态成员函数不能使用this指针！ class A{ int i; public: void hello_1(){cout&lt;&lt;\"hello\"&lt;&lt;endl;} //void hello_1(A * this){cout&lt;&lt\"hello\"&lt;&lt;endl;} void hello_2(){cout&lt;&lt;i&lt;&lt;\"hello\"&lt;&lt;endl;} //void hello_2(A * this){cout&lt;&lt;this-&gt;i&lt;&lt;\"hello\"&lt;&lt;endl; }; int main(){ A *p =NULL; p-&gt;hello_1(); //输出hello hello_1(p) p-&gt;hello_2(); //出错 return 0; } 常量对象 常量成员函数和常引用 如果不希望某个对象的值被改变，则定义对象的时候前面加const关键字。const Demo obj; //常量对象在类的成员函数说明后面加const，该成员函数成为常量成员函数。在常量成员数不能修改成员变量的值（除静态成员变量外），也不能调用同类的非常量成员函数（静态成员函数除外）。void Sample::GetValue() const;常量成员函数重载：两个成员函数，名字和参数都一样，但一个是const，一个不是，算重载。常引用：不能通过常引用修改其引用的变量。对象作为函数的参数时，生成该参数需要调用复制构造函数，效率比较低，用指针作参数，代码又不好看，这时可以用对象的常引用作为参数，就能确保不会修改对象的值。 运算符重载 运算符重载的实质是函数重载在程序编译时，把含运算符的表达式转换为对运算符函数的调用，把运算符的操作数转换给运算符函数的参数，运算符被多次重载时，根绝实参的类型决定调用哪个运算符函数。重载为普通函数时，参数个数为运算符目数；重载为类成员函数时，参数个数为运算符目数减一。 返回值类型 operator 运算符 (形参表){ } 赋值运算符‘=’重载 只能重载为成员函数返回值类型不能是void //一个长度可变的字符串类String // 包含一个char *类型的成员变量，指向动态分配的存储空间 //该存储空间用于存放'\\0'结尾的字符串 class String{ private: char * str; public: //构造函数，初始化str为NULL String (): str(NULL){} //c_str返回一个常量指针,确保不会通过这个指针修改对象的值 const char * c_str(){return str;} //实现String S1; S1=\"hello\";能够成立 char *operator = (const char * s); //实现String S1,S2; S1= \"this\"; S2=\"that\"; S1=S2;这里如果默认调用复制构造函数只能进行潜复制，存在很大问题 String & operator=(const String & s); //String s1; s1=\"hello\";String s2(s1);会调用复制构造函数，存在潜复制的问题 String(String & s); ~String(); }; //重载'=' char * String::operator=(const char *s){ if(str) delete [] str; //this指针 if(s){ //s不为NULL才执行拷贝 str = new char[strlen(s)+1]; strcpy(str, s); } else str = NULL; return str; } String & operator=(const String & s){ if(str == s.str) return * this; //String s; s= \"Hello”; s=s; if(str) delete [] str; if(s.str){ //s.str不为NULL才会执行拷贝 str = new char[strlen(s.str)+1]; strcpy(str, s.str); } else str = NULL; return * this; } //初始化的时候调用 String::String(String & s){ if(s.str){ str = new char[strlen(s.str)+1]; strcpy(str, s.str); } else str = NULL; } String s2 = \"hello\"; //出错，初始化，调用构造函数，但这个例子里面没有调用 运算符重载为友元函数 成员函数不能满足使用要求，例如不能通过对象调用成员函数实现某种操作普通函数不能访问类的私有成员 class Complex{ double real, imag; public: Complex(double r,double i): real(r), imag(i){}; Complex operator+(double r); friend Complex operator+(double r, const Complex & c); }; //能够计算c+5，但不能计算5+c Complex Complex::operator+(double r){ return Complex(real + r, imag); } //实现5+c的友元函数 Complex operator+ (double r, const Complex & c){ return Complex(c.real+r, c.imag); } 流插入和流提取运算符的重载 cout是在iostream中定义的ostream类的对象。 //重载为ostream的成员函数 ostream & ostream::operator&lt;&lt;(int n){ ... //输出n的代码 return * this; } ostream & ostream::operator&lt;&lt;(const char * s){ ... //输出s的代码 return * this; } cout&lt;&lt;5&lt;&lt;\"this\"; //函数调用形式如下 cout.operator&lt;&lt;(5).operator&lt;&lt;(\"this\"); //一个例子 #include &lt;iostream&gt; #include &lt;string&gt; #include &lt;cstdlib&gt; using namespace std; class Complex { double real, imag; public: Complex(double r=0,double i=0):real(r),imag(i){} friend ostream & operator&lt;&lt;(ostream & os, const Complex & c); friend istream & operator&gt;&gt;(istream & is, Complex & c); }; ostream & operator&lt;&lt;(ostream &os, const Complex & c){ //以“a+bi\"的形式输出 os&lt;&lt;c.real&lt;&lt;\"+\"&lt;&lt;c.imag&lt;&lt;\"i\"; return os; } istream & operator&gt;&gt;(istream & is,Complex & c){ string s; //将\"a+bi\"作为字符串读入，不能有空格 is&gt;&gt;s; int pos = s.find(\"+\",0); string sTmp = s.substr(0,pos); //分离出实部 //atof库函数能将const char *指针指向的内容转换成float c.real = atof(sTmp.c_str()); sTmp = s.substr(pos+1, s.length()-pos-2); //分离虚部 c.imag = atof(sTmp.c_str()); return is; } 自增/自减运算符重载 前置运算符作为一元运算符重载，后置作为二元运算符重载（多写一个参数，具体无意义） 类型强制转换运算符重载时，不能写返回值类型，实际上其返回值类型是类型强制转换运算符代表的类型 class CDemo{ private: int n; public: CDemo(int i=0):n(i){} CDemo operator++(); //前置形式 CDemo operator++(int); //后置形式 operator int(){return n;} //强制类型转换运算符重载，(int)s;等效于s.int(); friend CDemo operator--(CDemo &); //全局函数 friend CDemo operator--(CDemo &, int); //全局函数 }; CDemo CDemo::operator++(){//前置 n++; return *this; } CDemo CDemo::operator(int k){ CDemo tmp(*this); n++; return tmp; } CDemo operator--(CDemo &d){ d.n--; return d; } CDemo operator--(CDemo & d,int){ CDemo tmp(d); d.n--; return tmp; } 运算符重载注意事项： C++不允许定义新的运算符重载后运算符的含义应该符合日常习惯运算符重载不改变运算符的优先级以下运算符不能被重载: . .* :: ?: sizeof重载运算符(), [], -&gt;, =时，重载函数必须声明为类的成员函数","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://mtpgm.com/tags/C-C/"}]},{"title":"C语言学习","slug":"C-programming","date":"2016-12-17T10:40:08.000Z","updated":"2016-12-30T21:30:43.236Z","comments":true,"path":"2016/12/17/C-programming/","link":"","permalink":"http://mtpgm.com/2016/12/17/C-programming/","excerpt":"C语言备忘录 #include &lt;iostream&gt; using namespace std; int main(){ cout&lt;&lt\"hello, world\"&lt;&lt;endl; return 0; }","text":"C语言备忘录 #include &lt;iostream&gt; using namespace std; int main(){ cout&lt;&lt\"hello, world\"&lt;&lt;endl; return 0; } 函数 r=sqrt(100.0); //求平方根 k=pow(x, y); //求幂 x^y i=strlen(str1); //求字符串长度 v=strcmp(str1, str2); //比较两个字符串大小 n=atoi(str1); //字符串转换为整数 “123” to 123 函数原型(Signature) = 返回值类型+函数名+参数类型，里面的参数名（形参）可以不写。 #include &lt;iostream&gt; using namespace std; float max(float, float); int main(){ cout&lt;&lt;max(3, 4); return 0; } float max(float a, float b){ if(a &gt; b) return a; else return b; } 参数传递：实参与形参具有不同的存储单元，函数调用时，系统给形参分配存储单元，并将实参对应的值传递给形参，形参的值改变不会影响实参。 变量的作用域：根据变量在程序中作用范围的不同，可以将变量分为 局部变量： 在函数内或块内定义，只在这个函数或块内起作用的变量； 全局变量： 在所有函数外定义的变量，它的作用域是从定义变量的位置开始到本程序文件结束； 当全局变量与局部变量同名时，局部变量将在自己作用域内有效，它将屏蔽同名的全局变量。 #include &lt;iostream&gt; using namespace std; int a=0, b=0; //全局变量 void exchange(int a, int b){ //局部变量a, b int p; if (a &lt; b){ p=a; a=b; b=p; //a, b交换，不会影响前面a, b的值 } } int main(){ cin&gt;&gt;a&gt;&gt;b; exchange(a, b); cout&lt;&lt;a&lt;&lt;\" \"&lt;&lt;b&lt;&lt;endl; return 0; } 数组名是一个常量，用来存储数组在内存中的地址。 #include &lt;iostream&gt; using namespace std; void change(int a[]){ a[0]=30; a[1]=50; } int main(){ int a[2]={3,5}; change(a); //数组名作为参数是把数组的地址copy给了形参，这里将会改变数组元素的值 cout&lt;&lt;a[0]&lt;&lt;\" \"&lt;&lt;a[1]&lt;&lt;endl; return 0; } 递归 #include &lt;iostream&gt; using namespace std; int recur(){ char c; c = cin.get(); if (c != '\\n') recur(); cout&lt;&lt;c; return 0; } int main(){ recur(); return 0; } 输入abcd回车，将会输出回车dcba。每次输入不为回车，程序调用recur，当遇到回车符时，程序将不会再调用自身，从最后一个recur开始向下执行cout输出。 指针 取址运算符&amp; 指针运算符* int a=10; cout&lt;&lt;&a; //输出a在内存中的地址 cout&lt;&lt;*&a; //输出10 int *pointer; //定义一个指向整型变量的指针变量，里面存储一个地址，初始化需要赋值一个地址 指针变量： 用于存放指针（某个变量的地址）的变量 数组名代表数组首元素的地址，即相当于指向数组第一个元素的指针。数组名是常量，不能赋值。 用指针变量访问数组元素，给指针赋值数组的地址，指针可以像数组名一样访问数组元素。 #include &lt;iostream&gt; using namespace std; int main(){ int a[5]={10,11,12,13,14}; int *p =NULL; cout&lt;&lt;a&lt;&lt;endl; //输出地址 p = a; cout&lt;&lt;p&lt;&lt;endl; //输出地址 cout&lt;&lt;*p&lt;&lt;endl; //输出a[0] 10 cout&lt;&lt;*p++&lt;&lt;endl; //输出a[0] 10 后置++优先级高于*，但后置++是先用元素再自加 现在p指向a[1] cout&lt;&lt;*p++&lt;&lt;endl; //同上 输出a[1] 11 cout&lt;&lt;*p+1&lt;&lt;\"==\"&lt;&lt;p[1]&lt;&lt;\"==\"&lt;&lt;a[3]&lt;&lt;endl; //输出a[3] 13 return 0; } 递归实现斐波那契数列 #include &lt;iostream&gt; using namespace std; void feb(int n, int a[]){ if (n==1){ a[0]=1;a[1]=1; } if (n==2){ a[0]=1;a[1]=1; } else{ feb(n-1, a); int tmp = a[0]; a[0]=a[0]+a[1];a[1]=tmp; } } int main(){ int a[2]={0}; feb(10, a); cout&lt;&lt;\"n=10: \"&lt;&lt;a[0]&lt;&lt;endl; //55 return 0; } 指向二维数组的指针 #include &lt;iostream&gt; using namespace std; int main(){ int a[3][4]={1,2,3,4,5,6,7,8,9,10,11,12}; int *p; for (p=&a[0][0]; p&lt;&a[0][0]+12;p++){ cout&lt;&lt;p&lt;&lt;\" \"&lt;&lt;*p&lt;&lt;endl; } return 0; } 对于二维数组a，a相当于指向a[3][4]的“第一个元素”的指针，所谓的“第一个元素”是指一个“包含4个int型元素的一维数组”，所以a相当于一个“包含4个int型元素的一维数组”的地址。假如定义一个指针p，要使p=a，那么p的“基类型”应该是“包含4个int型元素的一维数组”，可写为int (*p)[4]，如下。 int a[3][4]={1,2,3,4,5,6,7,8,9,10,11,12}; int (*p)[4]; p=a; //赋值，*(*(p+i)+j) == p[i][j] == a[i][j] 分析*(*(p+i)+j) p指向一个“包含4个int型元素的一维数组”； p+i是第i+1个“包含4个int型元素的一维数组”的地址； p+i等价于&amp;a[i]； *(p+i)等价于a[i]; a[i]+j就是&amp;a[i][j]，而&amp;a[i]+j是&amp;a[i+j]; *(p+i)+j等价于a[i]+j，也就是&amp;a[i][j]； 所以((p+i)+j)等价于a[i][j]。 除非sizeof，Alignof，&amp;三个操作符作用于数组名，或者赋值为一个字符串，数组名将会被转换为指向首元素的指针。即int a[4]，假如int占4个字节，那么a+1将会返回数组首元素的地址加上4的地址；然而&amp;a是被当作更高一级的指向整个数组的指针，所以&amp;a+1将会返回数组首元素的地址加上4乘以数组长度的地址。 #include &lt;iostream&gt; using namespace std; int main(){ int a[4]={1,3,5,7}; cout&lt;&lt;a&lt;&lt;endl; cout&lt;&lt;a+1&lt;&lt;endl; //跨越一个int，即4个字节 cout&lt;&lt;&a&lt;&lt;endl; cout&lt;&lt;&a+1&lt;&lt;endl; //跨越整个数组，本例中的16个字节 cout&lt;&lt;*(&a)&lt;&lt;endl; //*(&a)相当于a return 0; } 数组名相当于指向数组第一个元素的指针，对于二维数组，第一个元素将是一个数组；&amp;E相当于把E的管辖范围上升了一个级别，体现在+1时跨越的字节数；*E相当于把E的管辖范围下降了一个级别，最小的时候是代表一个数值，int a[2]={1,2}; *a表示1. 指向字符串的指针 int main(){ char a[] = {'h','e','l','l','o','\\0'}; char *p = a; cout&lt;&lt;a&lt;&lt;endl; // hello cout&lt;&lt;p&lt;&lt;endl; // hello cout&lt;&lt;static_cast&lt;void*&gt;(a)&lt;&lt;endl; //输出地址 cout&lt;&lt;static_cast&lt;void*&gt;(p)&lt;&lt;endl; //输出地址 //p=\"ABC\"; //cout&lt;&lt;p&lt;&lt;endl; } 符号常量 const int a = 10; int const a = 10; 指向符号常量的指针 const int *p; 只是不能更改指针所指向的量的数值。 静态局部变量：函数中局部变量的值在函数调用结束后不消失而保留原值，即其占用的存储单元不释放，在下一次该函数调用时，仍可以继续使用该变量。 static int value = 20; 指针用作函数参数时为了防止对所指内容进行修改，可以用const来“限制”指针的功能；当指针用作函数返回值时，必须确保函数返回的地址是有意义的，返回全局变量或者静态局部变量的地址。 结构体 一种数据类型 struct student{ int id; char name[20]; float score; }; //分号 student student1,student2; student student3 = {3,{'m','i','k','e','\\0'}, 82.1}; student3.id = student3.id+3; student1=student3; student *one = student3; //指向结构体的指针 cout&lt;&lt;(*one).id&lt;&lt;\" \"&lt;&lt;(*one).name; cout&lt;&lt;one-&gt;id&lt;&lt;\" \"&lt;&lt;one-&gt;name; //指向运算符，指针操作结构体和其他的都一样。 student myclass[2]={1,{'q','\\0'},10,2,{'w','\\0'},2}; 动态申请内存空间 int *pint = new int(1024); //new 创建内存空间，并返回地址 delete pint; int *pia = new int[4]; delete [] pia;","categories":[],"tags":[{"name":"C/C++","slug":"C-C","permalink":"http://mtpgm.com/tags/C-C/"}]},{"title":"t-SNE:高维数据可视化","slug":"t-sne","date":"2015-08-17T14:26:30.000Z","updated":"2016-12-30T19:36:35.709Z","comments":true,"path":"2015/08/17/t-sne/","link":"","permalink":"http://mtpgm.com/2015/08/17/t-sne/","excerpt":"t-SNE，即t-distributed stochastic neighbor embedding，也是一种流体学习方法（manifold learning），通过保持数据点的相邻关系把数据从高维空间中降低到2维平面上，对高维数据可视化的效果非常好。本文使用python的机器学习包sklearn来对这个算法进行简单的介绍。","text":"t-SNE，即t-distributed stochastic neighbor embedding，也是一种流体学习方法（manifold learning），通过保持数据点的相邻关系把数据从高维空间中降低到2维平面上，对高维数据可视化的效果非常好。本文使用python的机器学习包sklearn来对这个算法进行简单的介绍。导入所需要的包，12345678910111213141516171819202122232425262728293031323334# That's an impressive list of imports.import numpy as npfrom numpy import linalgfrom numpy.linalg import normfrom scipy.spatial.distance import squareform, pdist# We import sklearn.import sklearnfrom sklearn.manifold import TSNEfrom sklearn.datasets import load_digitsfrom sklearn.preprocessing import scale# We'll hack a bit with the t-SNE code in sklearn 0.15.2.from sklearn.metrics.pairwise import pairwise_distancesfrom sklearn.manifold.t_sne import (_joint_probabilities, _kl_divergence)from sklearn.utils.extmath import _ravel# Random state.RS = 20150101# We'll use matplotlib for graphics.import matplotlib.pyplot as pltimport matplotlib.patheffects as PathEffects# We import seaborn to make nice plots.import seaborn as snssns.set_style('darkgrid')sns.set_palette('muted')sns.set_context(\"notebook\", font_scale=1.5, rc=&#123;\"lines.linewidth\": 2.5&#125;)# We'll generate an animation with matplotlib and moviepy.from moviepy.video.io.bindings import mplfig_to_npimageimport moviepy.editor as mpy 首先介绍一个可视化手写字体的例子。数据使用sklearn包里面带的一个数据集，共有1797张手写字体的图片，每张图片的像素为8*8=64。123456789101112digits = load_digits()print(digits.shape) #1797, 64print(digits['DESCR'])nrows, ncols = 2, 5plt.figure(figsize=(6,3))plt.gray()for i in range(ncols * nrows): ax = plt.subplot(nrows, ncols, i + 1) ax.matshow(digits.images[i,...]) plt.xticks([]); plt.yticks([]) plt.title(digits.target[i])plt.savefig('digits-generated.png', dpi=150) 接着运行t-SNE算法123456# We first reorder the data points according to the handwritten numbers.X = np.vstack([digits.data[digits.target==i] for i in range(10)])y = np.hstack([digits.target[digits.target==i] for i in range(10)])digits_proj = TSNE(random_state=RS).fit_transform(X) 写绘图函数，用转换后的数据点绘图。1234567891011121314151617181920212223242526def scatter(x, colors): # We choose a color palette with seaborn. palette = np.array(sns.color_palette(\"hls\", 10)) # We create a scatter plot. f = plt.figure(figsize=(8, 8)) ax = plt.subplot(aspect='equal') sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40, c=palette[colors.astype(np.int)]) plt.xlim(-25, 25) plt.ylim(-25, 25) ax.axis('off') ax.axis('tight') # We add the labels for each digit. txts = [] for i in range(10): # Position of each label. xtext, ytext = np.median(x[colors == i, :], axis=0) txt = ax.text(xtext, ytext, str(i), fontsize=24) txt.set_path_effects([ PathEffects.Stroke(linewidth=5, foreground=\"w\"), PathEffects.Normal()]) txts.append(txt) return f, ax, sc, txts 展示结果，可以看到不同数字被分成了不同的簇。12scatter(digits_proj, y)plt.savefig('images/digits_tsne-generated.png', dpi=120) #算法原理对每个数据点i，和每个潜在的邻居j，首先计算一个条件概率$p_{j|i}$，表示i选择j为邻居的概率：$$p_{j|i}=\\frac{exp(-||x_i-x_j||^2/2\\sigma_i ^2)}{\\sum_{k\\neq i}exp(-||x_i-x_k||^2/2\\sigma_i ^2)}$$$\\sigma_i$是以点$x_i$为中心的高斯分布的方差，是通过二分法找到的能使分布的熵等于$log(Perp)$，$Perp$是perplexity，用来衡量点$x_i$有效邻居的个数，值可以设置在5-50之间，是$\\sigma_i$的单调函数。$$Perp(P_i)=2^{H(P_i)} \\ \\ \\ \\ H(P_i)=-\\sum_j p_{j|i}log_2 p_{j|i}$$接着我们定义数据点间的相似性为对称的条件概率，$$p_{ij}=\\frac{p_{j|i}+p_{i|j}}{2N}$$ #相似性矩阵（similarity matrix）下面我们比较一下数据点的距离矩阵，$\\sigma$为常量的相似性矩阵和$\\sigma$为变量时的相似性矩阵。12345678910111213141516171819202122232425262728293031def _joint_probabilities_constant_sigma(D, sigma): P = np.exp(-D**2/2 * sigma**2) P /= np.sum(P, axis=1) return P# Pairwise distances between all data points.D = pairwise_distances(X, squared=True)# Similarity with constant sigma.P_constant = _joint_probabilities_constant_sigma(D, .002)# Similarity with variable sigma.P_binary = _joint_probabilities(D, 30., False)# The output of this function needs to be reshaped to a square matrix.P_binary_s = squareform(P_binary)plt.figure(figsize=(12, 4))pal = sns.light_palette(\"blue\", as_cmap=True)plt.subplot(131)plt.imshow(D[::10, ::10], interpolation='none', cmap=pal)plt.axis('off')plt.title(\"Distance matrix\", fontdict=&#123;'fontsize': 16&#125;)plt.subplot(132)plt.imshow(P_constant[::10, ::10], interpolation='none', cmap=pal)plt.axis('off')plt.title(\"$p_&#123;j|i&#125;$ (constant $\\sigma$)\", fontdict=&#123;'fontsize': 16&#125;)plt.subplot(133)plt.imshow(P_binary_s[::10, ::10], interpolation='none', cmap=pal)plt.axis('off')plt.title(\"$p_&#123;j|i&#125;$ (variable $\\sigma$)\", fontdict=&#123;'fontsize': 16&#125;)plt.savefig('similarity-generated.png', dpi=120) 可以看到数据已经呈现出10组，分别对应10个数字。现在使用1自由度的t-分布来定义低维空间数据点的相似性矩阵，$$q_{ij}=\\frac{(1+||y_i -y_j||^2)^{-1}}{\\sum_{k\\neq l}(1+||y_k-y_l||^2)^{-1}}$$我们的目的是找到低维空间的一组坐标使得这两个分布尽可能相似，一个自然的度量就是这两个分布的Kullback-Leiber divergence:$$C=KL(P||Q)=\\sum _{i,j}p_{ij} \\frac{p_{ij}}{q_{ij}}$$我们使用梯度下降最小化这个得分，原始论文中有详细的求导过程，这里我们只写出结果：$$\\frac{\\delta C}{\\delta y_i}=4\\sum _j (p_{ij}-q_{ij})(y_i -y_j)(1+||y_i -y_j||^2)^{-1}$$这个公式有很好物理意义。$p_{ij}-q_{ij}$为正，说明$x_i$在高维空间和$x_j$相似度高，而低维空间相似度低，$x_i$将会靠近$x_j$，反之则会远离$x_j$。下面用一个动画来阐明这个过程。首先monkey-patch sklearn包的t-SNE里面的_gradient_descent()函数，把每次迭代的结果保存下来。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# This list will contain the positions of the map points at every iteration.positions = []def _gradient_descent(objective, p0, it, n_iter, n_iter_without_progress=30, momentum=0.5, learning_rate=1000.0, min_gain=0.01, min_grad_norm=1e-7, min_error_diff=1e-7, verbose=0, args=[]): # The documentation of this function can be found in scikit-learn's code. p = p0.copy().ravel() update = np.zeros_like(p) gains = np.ones_like(p) error = np.finfo(np.float).max best_error = np.finfo(np.float).max best_iter = 0 for i in range(it, n_iter): # We save the current position. positions.append(p.copy()) new_error, grad = objective(p, *args) error_diff = np.abs(new_error - error) error = new_error grad_norm = linalg.norm(grad) if error &lt; best_error: best_error = error best_iter = i elif i - best_iter &gt; n_iter_without_progress: break if min_grad_norm &gt;= grad_norm: break if min_error_diff &gt;= error_diff: break inc = update * grad &gt;= 0.0 dec = np.invert(inc) gains[inc] += 0.05 gains[dec] *= 0.95 np.clip(gains, min_gain, np.inf) grad *= gains update = momentum * update - learning_rate * grad p += update return p, error, isklearn.manifold.t_sne._gradient_descent = _gradient_descentX_proj = TSNE(random_state=RS).fit_transform(X)X_iter = np.dstack(position.reshape(-1, 2) for position in positions) 使用moviepy包生成gif动画123456789101112131415f, ax, sc, txts = scatter(X_iter[..., -1], y)def make_frame_mpl(t): i = int(t*40) x = X_iter[..., i] sc.set_offsets(x) for j, txt in zip(range(10), txts): xtext, ytext = np.median(x[y == j, :], axis=0) txt.set_x(xtext) txt.set_y(ytext) return mplfig_to_npimage(f)animation = mpy.VideoClip(make_frame_mpl, duration=X_iter.shape[2]/40.)animation.write_gif(\"d:/dataVisualization/animation.gif\", fps=20) 接着创建低维空间数据点的相似性矩阵的动画123456789101112131415161718192021n = 1. / (pdist(X_iter[..., -1], \"sqeuclidean\") + 1)Q = n / (2.0 * np.sum(n))Q = squareform(Q)f = plt.figure(figsize=(6, 6))ax = plt.subplot(aspect='equal')im = ax.imshow(Q, interpolation='none', cmap=pal)plt.axis('tight')plt.axis('off')def make_frame_mpl(t): i = int(t*40) n = 1. / (pdist(X_iter[..., i], \"sqeuclidean\") + 1) Q = n / (2.0 * np.sum(n)) Q = squareform(Q) im.set_data(Q) return mplfig_to_npimage(f)animation = mpy.VideoClip(make_frame_mpl, duration=X_iter.shape[2]/40.)animation.write_gif(\"d:/dataVisualization/animation_matrix.gif\", fps=20) 可以看到相似性矩阵越来越接近原始数据的相似性矩阵。 #t-分布下面解释一下选择1自由度t-分布的原因。一个半径为r的N维球体的体积与$r^N$成正比，当N非常大的时候，如果我们在这个球体中以均匀分布选择一些点，那么大多数点都会非常靠近球的表面，非常少的点会在中心附近。可以用模拟的方法阐释这个现象：1234567891011121314151617181920npoints = 1000plt.figure(figsize=(15, 4))for i, D in enumerate((2, 5, 10)): # Normally distributed points. u = np.random.randn(npoints, D) # Now on the sphere. u /= norm(u, axis=1)[:, None] # Uniform radius. r = np.random.rand(npoints, 1) # Uniformly within the ball. points = u * r**(1./D) # Plot. ax = plt.subplot(1, 3, i+1) ax.set_xlabel('Ball radius') if i == 0: ax.set_ylabel('Distance from origin') ax.hist(norm(points, axis=1), bins=np.linspace(0., 1., 50)) ax.set_title('D=%d' % D, loc='left')plt.savefig('spheres-generated.png', dpi=100, bbox_inches='tight') 如果我们对高维空间中原始数据和低维空间映射的数据点使用相同的高斯分布，则会造成数据点与其邻居间距离分布的不平衡。而当算法想要在这两个空间中重复出相同的距离时，这种不平衡就会造成数据点间过度的吸引力，使数据点多靠近中心位置。而在低维空间使用1自由度的t-分布可以避免这个问题。1自由度的t-分布较高斯分布有更高尾部，补偿了因空间维度造成的距离分布的不平衡。12345678z = np.linspace(0., 5., 1000)gauss = np.exp(-z**2)cauchy = 1/(1+z**2)plt.figure()plt.plot(z, gauss, label='Gaussian distribution')plt.plot(z, cauchy, label='Cauchy distribution')plt.legend()plt.savefig('distributions-generated.png', dpi=100) 使用这个分布会得到更有效的数据可视化，不同的簇会有更明显的区分。 #参考文献： van der Maaten, L. and Hinton, G. E. (2008). Visualizing data usingt-SNE. J. Machine Learning Res., 9. https://beta.oreilly.com/learning/an-illustrated-introduction-to-the-t-sne-algorithm","categories":[],"tags":[{"name":"data visualization","slug":"data-visualization","permalink":"http://mtpgm.com/tags/data-visualization/"}]},{"title":"非线性维度下降之局部线性插值","slug":"lle","date":"2015-08-14T01:18:41.000Z","updated":"2016-12-30T18:17:44.152Z","comments":true,"path":"2015/08/14/lle/","link":"","permalink":"http://mtpgm.com/2015/08/14/lle/","excerpt":"非线性维度下降的目的是发现隐藏在高维数据中的低维结构，例如手写字体的方向，弯曲程度，写字风格（比如2右下角带不带环）。维度下降的基本假设是数据点位于高维空间中的一个很薄的流体上或者其附近，这个流体的维度就是数据的内在维度，远低于空间的维度，维度下降算法就是用来重构这个内在维度。","text":"非线性维度下降的目的是发现隐藏在高维数据中的低维结构，例如手写字体的方向，弯曲程度，写字风格（比如2右下角带不带环）。维度下降的基本假设是数据点位于高维空间中的一个很薄的流体上或者其附近，这个流体的维度就是数据的内在维度，远低于空间的维度，维度下降算法就是用来重构这个内在维度。线性维度下降，例如PCA, ICA, 是假设这个流体是一个低维平面，即一组正交的基向量生成的一个空间，数据点可以由这些基向量的不同线性组合来表示，非线性维度下降则是应对非这种平面的流体的一种降维方法，一般称为流体学习（mandifold learning）。本文介绍一种局部线性插值（locally linear embedding）的流体学习方法及它的一个改进。这个方法有很好的几何直观性，数据点在高维空间如果能由它的邻近数据点的加权得到，那么在低维空间同样可以用相同的权重由它的邻近数据点加权得到。算法的推导和实现如下： 计算k-近邻找到每个数据点K-个最近的邻近数据点，K的选择影响算法最后得到的低维结构。python code: 12345678import numpy as npfrom numpy.matlib import repmat#X = data as D x N matrix (D = dimensionality, N = #points)D,N=X.shapeX2=np.sum(X**2,0).reshape(1,N)distance=repmat(X2,N,1)+repmat(X2.T,1,N)-2*np.dot(X.T,X)index=np.argsort(distance,0)neighborhood=index[1:1+K,:] 计算权重矩阵对每个数据点$\\mathbf{x}$，K个最近邻用$\\mathbf{x}_\\boldsymbol{j}$表示，最小化它的重构误差，$$\\varepsilon(\\mathbf{w})=||\\mathbf{x}-\\sum_j w _j \\mathbf{x}_\\boldsymbol{j} ||^2 \\ \\ \\ \\ s.t. \\ \\ \\sum _j w_j = 1$$$$\\varepsilon(\\mathbf{w})=||\\sum _j w _j (\\mathbf{x}-\\mathbf{x}_\\boldsymbol{j})||^2 = \\sum _ {jk} w _j w _k (\\mathbf{x}-\\mathbf{x}_\\boldsymbol{j})^T (\\mathbf{x}-\\mathbf{x}_\\boldsymbol{k})$$令$\\mathbf{G}=[\\centerdot\\centerdot\\centerdot, (\\mathbf{x}-\\mathbf{x}_\\boldsymbol{j}),\\centerdot\\centerdot\\centerdot]$，可定义局部协方差矩阵$\\mathbf{C}$（local covariance matrix）为$$\\mathbf{C} = \\mathbf{G}^T \\mathbf{G} $$此时有$$\\varepsilon(\\mathbf{w})=\\mathbf{w}^\\boldsymbol{T} \\mathbf{C} \\mathbf{w} \\ \\ \\ s.t.\\ \\ ||\\mathbf{w}||=1$$可以利用拉格朗日乘数法求最优解，构造函数$$f(\\mathbf{w},\\lambda)=\\mathbf{w}^\\boldsymbol{T} \\mathbf{C} \\mathbf{w} - 2*\\lambda (\\boldsymbol{1}^T \\mathbf{w}-1)$$求偏导并令之等于零可得，$$\\mathbf{C} \\mathbf{w}=\\lambda\\boldsymbol{1}$$根据$ \\mathbf{w}$的模为1可解出$\\lambda=\\frac{1}{\\boldsymbol{1}^T \\mathbf{C}^{-1} \\boldsymbol{1}}$，即矩阵$\\mathbf{C}$的逆所有元素相加的和。进而可以求出$\\mathbf{w}$$$\\mathbf{w}=\\frac{\\mathbf{C}^{-1}\\boldsymbol{1}}{\\boldsymbol{1}^T \\mathbf{C}^{-1} \\boldsymbol{1}}$$可以看出分母是分子的和，所以等同于求$\\mathbf{C}\\mathbf{w}=\\boldsymbol{1}$的解，然后归一化使$\\mathbf{w}$的模为1。当K大于数据点的维度时，每个数据点可以由不同的邻近数据点的不同组和线性表示出来，即权重矩阵不唯一，矩阵$\\mathbf{C}$为奇异矩阵，无法求逆，此时应加入正则项。python code: 1234567891011if K&gt;D: tol=1e-3else: tol=0W=np.zeros((K,N))for ii in range(N): z=X[:,neighborhood[:,ii]]-repmat(X[:,ii].reshape(D,1),1,K) C=np.dot(z.T,z) C=C+np.eye(K)*tol*np.trace(C) W[:,ii]=np.dot(np.linalg.inv(C),np.ones((K,1))).reshape((12,)) W[:,ii]=W[:,ii]/sum(W[:,ii]) 求低维空间的坐标最小化插值误差函数\\[\\begin{split}\\Phi(\\mathbf{Y})&amp;=\\sum _i ||\\mathbf{y} _i - \\sum _j w _j \\mathbf{y} _j||^2=tr((\\mathbf{Y}-\\mathbf{WY})^T(\\mathbf{Y}-\\mathbf{WY})) \\\\ &amp;=tr(\\mathbf{Y}^T(\\mathbf{I}-\\mathbf{W})^T(\\mathbf{I}-\\mathbf{W})\\mathbf{Y}) =\\sum _i ^d \\mathbf{Y}_i ^T \\mathbf{M}\\mathbf{Y} _i \\end{split}\\]令$\\mathbf{M}=(\\mathbf{I}-\\mathbf{W})^T(\\mathbf{I}-\\mathbf{W})$，并且约束$\\mathbf{Y}$有零均值，单位协方差矩阵，那么最小化此二次型当且仅当$\\mathbf{Y}$为$\\mathbf{M}$的最小d+1个特征值对应的特征向量（Rayleigh-Ritz theorem），舍去第一个零特征值对应的特征向量。python code: 123456789101112import scipy.sparse as spfrom scipy.sparse.linalg.eigen.arpack import eigshM=sp.csr_matrix(np.eye(N))for ii in range(N): w=W[:,ii] jj=neighborhood[:,ii] M[ii,jj]=M[ii,jj]-w M[jj,ii]=M[jj,ii]-w.reshape(K,1) M[np.ix_(jj,jj)]=M[np.ix_(jj,jj)]+np.dot(w.reshape(K,1),w.reshape(1,K))print 'begin solving eigenvectors'eigenvals, Y=eigsh(M,d+1,sigma=0.0,tol=1e-6)Y_r=Y[:,1:].T*(np.sqrt(N)) 以上各部分可整合成一个函数lle(X,K,d)，X是输入数据，K是近邻数，d是流体的维度。下面对瑞士卷(swissroll)和S-曲线(s-curve)使用lle算法。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107N=2000K=12d=2tt0 = (3*np.pi/2)*(1+2*np.arange(0,1.01,0.02))hh = np.arange(0,1.01,0.125)*30xx = np.dot((tt0*np.cos(tt0)).reshape(len(tt0),1),np.ones((1,len(hh))))yy = np.dot(np.ones((len(tt0),1)),hh.reshape(1,len(hh)))zz = np.dot((tt0*np.sin(tt0)).reshape(len(tt0),1),np.ones((1,len(hh))))cc = np.dot(tt0.reshape(len(tt0),1),np.ones((1,len(hh))))from mpl_toolkits.mplot3d import Axes3Dfrom matplotlib import cmimport matplotlib.pyplot as pltfrom matplotlib.colors import Normalizecolor_map = cm.jetscalarMap = cm.ScalarMappable(norm=Normalize(vmin=cc.min(), vmax=cc.max()), cmap=color_map)C_colored = scalarMap.to_rgba(cc)fig = plt.figure()ax = fig.add_subplot(2,3,1,projection='3d')ax._axis3don = Falsesurf = ax.plot_surface(xx, yy, zz, rstride=1, cstride=1,facecolors=C_colored,zorder=1)surf.set_edgecolor('k')ax.view_init(20,-72)ax.set_xlim(-15,20)ax.set_ylim(0,32)ax.set_zlim(-15,15)ax.plot([-15,-15],[0,32],[-15,-15],'k-',lw=2,clip_on=True,clip_box=surf,zorder=-1)ax.plot([-15,20],[0,0],[-15,-15],'k-',linewidth=2)ax.plot([-15,-15],[0,0],[-15,15],'k-',linewidth=2)tt = (3*np.pi/2)*(1+2*np.random.rand(1,N))height = 21*np.random.rand(1,N)X = np.row_stack((tt*np.cos(tt),height,tt*np.sin(tt)))ax=fig.add_subplot(2,3,2,projection='3d')sca=ax.scatter(X[0,:],X[1,:],X[2,:],s=12,c=tt,marker='+',cmap=cm.jet, norm=Normalize(vmin=tt.min(), vmax=tt.max()))ax.view_init(20,-72)ax._axis3don=Falseax.set_xlim(-15,20)ax.set_ylim(0,32)ax.set_zlim(-15,15)ax.plot([-15,-15],[0,32],[-15,-15],'k-',lw=2,zorder=-1)ax.plot([-15,20],[0,0],[-15,-15],'k-',linewidth=2)ax.plot([-15,-15],[0,0],[-15,15],'k-',linewidth=2)Y=lle(X,K,d)ax=fig.add_subplot(2,3,3)ax.scatter(Y[0,:],Y[1,:],s=12,c=tt,marker='+',cmap=cm.jet, norm=Normalize(vmin=tt.min(), vmax=tt.max()))ax.spines['right'].set_visible(False)ax.spines['top'].set_visible(False)plt.xticks([])plt.yticks([])#S-curvett=np.arange(-1,0.51,0.1)*np.piuu=np.arange(0.5,-1.1,-0.1)*np.pihh=np.arange(0,1.01,0.1)*5xx=np.dot(np.row_stack((np.cos(tt), -1*np.cos(uu))).reshape(32,1),np.ones((1,11)))yy=np.dot(np.ones((32,1)),hh.reshape(1,11))zz=np.dot(np.row_stack((np.sin(tt), 2-np.sin(uu))).reshape(32,1),np.ones((1,11)))cc=np.dot(np.row_stack((tt,uu)).reshape(32,1),np.ones((1,11)))color_map = cm.jetscalarMap = cm.ScalarMappable(norm=Normalize(vmin=cc.min(), vmax=cc.max()), cmap=color_map)C_colored = scalarMap.to_rgba(cc)ax = fig.add_subplot(2,3,4,projection='3d')ax._axis3don = Falsesurf = ax.plot_surface(xx, yy, zz, rstride=1, cstride=1, facecolors=C_colored,zorder=1)surf.set_edgecolor('k')ax.view_init(10,-70)ax.set_xlim(-1,1)ax.set_ylim(0,5)ax.set_zlim(-1,3)angle=np.pi*(1.5*np.random.rand(1,N/2)-1)height=5*np.random.rand(1,N)X=np.row_stack((np.row_stack((np.cos(angle),-1*np.cos(angle))).reshape(1,N), height, np.row_stack((np.sin(angle),2-np.sin(angle))).reshape(1,N)))ax=fig.add_subplot(2,3,5,projection='3d')ax.scatter(X[0,:],X[1,:],X[2,:],s=12,c=np.row_stack((angle,angle)).reshape(1,N), marker='+',cmap=cm.jet, norm=Normalize(vmin=angle.min(), vmax=angle.max()))ax.view_init(10,-70)ax._axis3don=Falseax.set_xlim(-1,1)ax.set_ylim(0,5)ax.set_zlim(-1,3)Y=lle(X,K,d)ax=fig.add_subplot(2,3,6)ax.scatter(Y[1,:],Y[0,:],s=12,c=np.row_stack((angle,angle)).reshape(1,N),marker='+',cmap=cm.jet, norm=Normalize(vmin=angle.min(), vmax=angle.max()))ax.spines['right'].set_visible(False)ax.spines['top'].set_visible(False)plt.xticks([])plt.yticks([])plt.show() 结果如下：lle算法得到的低维结构有些变形，如上图所示，瑞士卷不是长方形的而是从宽到窄，相对而言，S型曲线的结果要好一些。下面介绍modified locally linear embedding算法来解决这个问题。lle算法计算权重矩阵时用到的重构误差函数为$||\\mathbf{Gw}||$，理论上$\\boldsymbol{1}$在矩阵$\\mathbf{G}$的零空间上的正交投影都可以归一化为符合要求的权重向量，那么权重最优解的近似值的个数为矩阵$\\mathbf{G}$零空间的维度，可以利用这样的多个权重矩阵描述每个数据点的局部结构，然后来求数据的低维表示。程序参照了sklearn包里面的locally_linear_embedding函数，如下，123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596from scipy.linalg import eigh, svd, qr, solvedef mlle(X,K,d,modified_tol=1E-12): D,N=X.shape #STEP1: COMPUTE PAIRWISE DISTANCES &amp; FIND NEIGHBORS X2=np.sum(X**2,0).reshape(1,N) distance=repmat(X2,N,1)+repmat(X2.T,1,N)-2*np.dot(X.T,X) index=np.argsort(distance,0) neighbors=index[1:1+K,:].T V = np.zeros((N, K, K)) nev = min(D, K) evals = np.zeros([N, nev]) use_svd = (K &gt; D) X=X.T if use_svd: for i in range(N): X_nbrs = X[neighbors[i]] - X[i] V[i], evals[i], _ = svd(X_nbrs, full_matrices=True) evals **= 2 else: for i in range(N): X_nbrs = X[neighbors[i]] - X[i] C_nbrs = np.dot(X_nbrs, X_nbrs.T) evi, vi = eigh(C_nbrs) evals[i] = evi[::-1] V[i] = vi[:, ::-1] reg = 1E-3 * evals.sum(1) tmp = np.dot(V.transpose(0, 2, 1), np.ones(K)) tmp[:, :nev] /= evals + reg[:, None] tmp[:, nev:] /= reg[:, None] w_reg = np.zeros((N, K)) for i in range(N): w_reg[i] = np.dot(V[i], tmp[i]) w_reg /= w_reg.sum(1)[:, None] rho = evals[:, d:].sum(1) / evals[:, :d].sum(1) eta = np.median(rho) s_range = np.zeros(N, dtype=int) evals_cumsum = np.cumsum(evals, 1) eta_range = evals_cumsum[:, -1:] / evals_cumsum[:, :-1] - 1 for i in range(N): s_range[i] = np.searchsorted(eta_range[i, ::-1], eta) s_range += K - nev M = np.zeros((N, N), dtype=np.float) for i in range(N): s_i = s_range[i] #select bottom s_i eigenvectors and calculate alpha Vi = V[i, :, K - s_i:] alpha_i = np.linalg.norm(Vi.sum(0)) / np.sqrt(s_i) #compute Householder matrix which satisfies # Hi*Vi.T*ones(n_neighbors) = alpha_i*ones(s) # using prescription from paper h = alpha_i * np.ones(s_i) - np.dot(Vi.T, np.ones(K)) norm_h = np.linalg.norm(h) if norm_h &lt; modified_tol: h *= 0 else: h /= norm_h #Householder matrix is # &gt;&gt; Hi = np.identity(s_i) - 2*np.outer(h,h) #Then the weight matrix is # &gt;&gt; Wi = np.dot(Vi,Hi) + (1-alpha_i) * w_reg[i,:,None] #We do this much more efficiently: Wi = (Vi - 2 * np.outer(np.dot(Vi, h), h) + (1 - alpha_i) * w_reg[i, :, None]) #Update M as follows: # &gt;&gt; W_hat = np.zeros( (N,s_i) ) # &gt;&gt; W_hat[neighbors[i],:] = Wi # &gt;&gt; W_hat[i] -= 1 # &gt;&gt; M += np.dot(W_hat,W_hat.T) #We can do this much more efficiently: nbrs_x, nbrs_y = np.meshgrid(neighbors[i], neighbors[i]) M[nbrs_x, nbrs_y] += np.dot(Wi, Wi.T) Wi_sum1 = Wi.sum(1) M[i, neighbors[i]] -= Wi_sum1 M[neighbors[i], i] -= Wi_sum1 M[i, i] += s_i M=sp.csr_matrix(M) print 'begin solving eigenvectors' eigenvals, Y=eigsh(M,d+1,sigma=0.0,tol=1e-12) Y_r=Y[:,1:].T*(np.sqrt(N)) return Y_r 用mlle算法重新跑瑞士卷和S-曲线的数据，结果如下，得到的2维结构正好是长方形。参考文献： S. Roweis and L Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290: 2323–2326, 2000. Zhang, Z. &amp; Wang, J. MLLE: Modified Locally Linear Embedding Using Multiple Weights. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382 http://www.cs.nyu.edu/~roweis/lle/ http://scikit-learn.org/stable/modules/generated/sklearn.manifold.locally_linear_embedding.html","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://mtpgm.com/tags/机器学习/"}]},{"title":"分类型变量的聚类算法 - ROCK","slug":"rock","date":"2015-07-10T15:40:22.000Z","updated":"2016-12-30T18:17:07.137Z","comments":true,"path":"2015/07/10/rock/","link":"","permalink":"http://mtpgm.com/2015/07/10/rock/","excerpt":"在数据挖掘中，聚类是发现数据分布模式的一种方式，把一组数据点按照某种合适的距离分成不同的簇，使得簇内有尽可能小的距离，而簇间有尽可能大的距离。本篇介绍一种适用于分类型变量的聚类算法 - ROCK，内容主要基于论文“ROCK: A Robust Clustering Algorithm for Categorical Attributes”。","text":"在数据挖掘中，聚类是发现数据分布模式的一种方式，把一组数据点按照某种合适的距离分成不同的簇，使得簇内有尽可能小的距离，而簇间有尽可能大的距离。本篇介绍一种适用于分类型变量的聚类算法 - ROCK，内容主要基于论文“ROCK: A Robust Clustering Algorithm for Categorical Attributes”。ROCK是一种基于图的聚类算法，定义了数据点间的邻居（neighbors）和链接（links）。邻居（neighbors）：两个数据点间的相似度大于一定的阈值就可以称为邻居。即$sim(p_i,p_j)\\geqslant\\theta$。其中$\\theta$是取值在0，1之间的一个小数，用于衡量两个点相似度。$sim$是相似度函数，可以是距离函数（如$L_1$, $L_2$），也可以是一些非度量型的（如一个领域的专家提供的相似度衡量方法）。对于分类型变量，我们选择Jaccard相似性函数，即$$sim(T_1, T_2)=\\frac{|T_1\\cap T_2|}{|T_1\\cup T_2|}$$对于分类型变量，我们把每条记录当作一个交易，如果某个属性值缺失，仅仅让这条交易不包含这个属性。当然这只是一种处理缺失值的方法，针对不同的数据会有不同的处理方式。Python code:123456789101112131415161718192021import numpy as np def neighbors(filename, theta): S=[] P=[] for line in open(filename): lines=line.rstrip().split(',') for i in range(1,len(lines)): lines[i]+=str(i) #print lines if '?11' in lines: lines.remove('?11') S.append(set(lines[1:])) P.append(lines[0]) A=np.zeros((len(S),len(S)),dtype='int16') for i in range(len(S)-1): for j in range(i+1,len(S)): if float(len(S[i].intersection(S[j])))/len(S[i].union(S[j])) &gt;= theta: A[i,j]=1 A[j,i]=1 return A,P 数据是UCI数据库里面用作benchmark的Mushroom数据集(https://archive.ics.uci.edu/ml/datasets/Mushroom)，其中只有一个一个属性值有缺失。使用上面的函数建立neighbors矩阵B和类别向量P。Python code:1B,P=neighbors('agaricus-lepiota.data.txt',0.8) 连接（links）：两个点之间的连接数$link(p_i, p_j)$定义为这两个点共同邻居的个数，因此，这种基于连接的方法是一种考虑全局性的聚类算法。通过下面程序可以得到Mushroom数据点之间的连接矩阵A。Python code:12345678910def compute_links(A): link=np.zeros_like(A) for i in range(A.shape[0]): N=np.where(A[i,]&gt;0)[0] for j in range(len(N)-1): for z in range(j+1,len(N)): link[N[j],N[z]]+=1 link[N[z],N[j]]+=1 return linkA=compute_links(B) 准则函数（Criterion Function）：寻找最好的簇等价于最大化准则函数。我们的目的是让簇内有高的连接度，同时最小化不同簇之间的连接度。因此可以采用如下函数：$$E_l = \\sum_{i=1}^k n_i \\ast \\sum_{p_q,p_r \\in C_i} \\frac{link(p_q, p_r)}{n_i^{1+2 f ( \\theta )}}$$ 其中$C_i$代表第i个簇，左边的分母为$C_i$中期望的连接数。本文使用的$f(\\theta)$为$\\frac{1-\\theta}{1+\\theta}$。 适合度函数（Goodness Measure）：根据准则函数，我们可以得到两个簇之间的适合度函数，即评价两个簇是否相近，以便合并在一起。如下：$$link[C_i, C_j] = \\sum_{p_q \\in C_i, p_r \\in C_j} link(p_q, p_r)$$ 即两个簇之间的连接数为其中结点间连接数的总和。 $$g(C_i,C_j)=\\frac{link[C_i,C_j]}{(n_i + n_j)^{1+2 f(\\theta)} - n_i^{1+2f(\\theta)}-n_j^{1+2f(\\theta)}}$$$C_i$, $C_j$两个簇合并在一起后期望的连接数为$(n_i +n_j)^{1+2f(\\theta)}$，每个簇内的期望数分别为$n_i^{1+2f(\\theta)}$和$n_j^{1+2f(\\theta)}$，因此簇间的期望连接数为他们相减。除以期望数可以让簇以使准则函数最大的方向进行。Python code:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293def goodness(value, c1, c2, theta): return value/((c1+c2)**(1+2*(1-theta)/(1+theta))-c1**(1+2*(1-theta)/(1+theta))-c2**(1+2*(1-theta)/(1+theta))) def delete(Q,value): for i in Q: if i[1]==value: Q.remove(i) breakdef update(Q,value,link): for i in range(len(Q)): if Q[i][1]==value: Q.remove(Q[i]) Q.insert(i,(link,value)) breakdef build_heap(A,theta): q=&#123;&#125; Q=[] for i in range(A.shape[0]): q[tuple([i])]=[] for j in range(A.shape[1]): if j==i or A[i,j]==0: continue #if tuple([i]) not in q: # q[tuple([i])]=[] heapq.heappush(q[tuple([i])], (goodness(A[i,j],1,1,theta)*(-1),[j])) #print q[tuple([i])] if len(q[tuple([i])])&gt;0: heapq.heappush(Q, (q[tuple([i])][0][0],[i])) return q,Q import heapqimport copy def cluster(A, k, theta): q=&#123;&#125; Q=[] for i in range(A.shape[0]): q[tuple([i])]=[] for j in range(A.shape[1]): if j==i or A[i,j]==0: continue heapq.heappush(q[tuple([i])], (goodness(A[i,j],1,1,theta)*(-1),[j])) if len(q[tuple([i])])&gt;0: heapq.heappush(Q, (q[tuple([i])][0][0],[i])) print len(Q) while len(Q)&gt;k: print \"lenQ: \",len(Q) print \"lenq: \",len(q) if Q[0][0] == 0: return Q u=heapq.heappop(Q) print \"lenQafterpop: \",len(Q) w=copy.deepcopy(u[1]) print \"MAXlink: \",u[0] u=u[1] if tuple(u) not in q: print \"u: \",u v=q[tuple(u)][0][1] print \"v: \",v delete(Q,v) print \"lenQafterdeletev: \",len(Q) print \"lenQafterremoveUV: \",len(Q) w.extend(v) q[tuple(w)]=[] U=[tuple(x[1]) for x in q[tuple(u)]] V=[tuple(x[1]) for x in q[tuple(v)]] U.extend(V) if len(U)==2: heapq.heappush(Q,(0,w)) continue for x in set(U): if set(x).issubset(set(tuple(w))) : continue link=0 for y in w: for z in x: link+=A[y,z] if tuple(x) not in q: print \"x: \",x delete(q[tuple(x)],u) delete(q[tuple(x)],v) g=goodness(link,len(x),len(w),theta)*(-1) q[tuple(x)].append((g,w)) q[tuple(w)].append((g,list(x))) heapq.heapify(q[tuple(x)]) heapq.heapify(q[tuple(w)]) update(Q,list(x),q[tuple(x)][0][0]) if len(q[tuple(w)])&gt;0: Q.append((q[tuple(w)][0][0],w)) heapq.heapify(Q) del q[tuple(u)] del q[tuple(v)] return Q 函数需要提供类的个数，当类达到要求时会结束聚类过程。当任何两个类之间没有共同连接时也会停止聚类。如下得到Mushroom数据聚类的结果。1Q=cluster(A,20,0.8) 最后查看所得到类的性质。1234567def printQ(Q,P): for i in range(len(Q)): stat=&#123;'p':0,'e':0&#125; for j in Q[i][1]: stat[P[j]]+=1 print str(i+1)+': p-'+str(stat['p'])+' e-'+str(stat['e'])printQ(Q,P) 输出如下：1: p-256 e-02: p-0 e-7043: p-0 e-17284: p-0 e-965: p-0 e-966: p-192 e-07: p-288 e-08: p-1728 e-09: p-0 e-19210: p-0 e-76811: p-32 e-012: p-36 e-013: p-8 e-014: p-0 e-19215: p-0 e-4816: p-0 e-28817: p-0 e-4818: p-72 e-3219: p-0 e-1620: p-8 e-021: p-1296 e-0可以看到，虽然指定了类的个数20，但算法找到了21个类，它们之间的连接数均为零，使得聚类过程无法继续进行下去。另外，可以看到，除了第18个簇，其他簇全部是有毒的(p)，或者全部是可食用的(e)，说明这种聚类算法在Mushroom数据集上得到了很好的结果。","categories":[],"tags":[{"name":"数据挖掘","slug":"数据挖掘","permalink":"http://mtpgm.com/tags/数据挖掘/"}]},{"title":"测度论（一）","slug":"measure","date":"2015-06-16T15:57:34.000Z","updated":"2016-12-30T18:17:27.966Z","comments":true,"path":"2015/06/16/measure/","link":"","permalink":"http://mtpgm.com/2015/06/16/measure/","excerpt":"测度论可以说是一门语言，有一些基本的概念，下面做一个简单的介绍。","text":"测度论可以说是一门语言，有一些基本的概念，下面做一个简单的介绍。样本空间：我们看到一个随机现象，把每个可能的结果当作空间中的一个点。空间需要足够的大，以便容纳模型的复杂度，同时需要足够的小，以便我们能够处理。大多时候，选择都是很容易的。例如，当我们想要对一个硬币的一次投掷的结果进行建模时，空间$\\Omega$就只有两个点在里面，即T和H；当硬币被投掷三次，则空间就变成长度为3的T，H的8个组合；当我们不停地投掷时，$\\Omega$就成为了所有H和T组成的无限序列的空间。 事件：事件是那些能被问结果是发生还是未发生的问题，是$\\Omega$的子集。对应结果的集合能够导致回答“是”。不是所有$\\Omega$的子集都是事件。其中一些是，它们组成了一个集族${F}$，是一个$\\Sigma$-域。因为不可测集不能求测度，被排除了。 概率：也叫测度，是一个定义在${F}$上的集合函数，也就是给事件赋予一个概率的函数。具有可列可加性。 随机变量：定义在样本空间上的实值函数。","categories":[],"tags":[{"name":"测度论","slug":"测度论","permalink":"http://mtpgm.com/tags/测度论/"}]},{"title":"非负矩阵分解（NMF）","slug":"nmf","date":"2015-06-12T15:12:26.000Z","updated":"2016-12-30T19:35:31.155Z","comments":true,"path":"2015/06/12/nmf/","link":"","permalink":"http://mtpgm.com/2015/06/12/nmf/","excerpt":"非负矩阵分解能够学习到对象的局部特征，例如把人脸分解成嘴巴，眼睛，鼻子等等不同的部分，在很多领域都有重要的应用，例如文本聚类，语音处理，协同过滤。其主要思想是把一个大的非负矩阵X分解为两个小矩阵（WH）的乘积，满足这两个小矩阵所有元素都不为负值的条件，即X=WH。然而完全相等很难实现，所以我们只要求能够充分接近就可以了。这时需要一个代价函数（cost function）来表示两者的差距。一种代价函数是基于欧式距离的，一种是分离度的，两个都有对应的迭代公式。","text":"非负矩阵分解能够学习到对象的局部特征，例如把人脸分解成嘴巴，眼睛，鼻子等等不同的部分，在很多领域都有重要的应用，例如文本聚类，语音处理，协同过滤。其主要思想是把一个大的非负矩阵X分解为两个小矩阵（WH）的乘积，满足这两个小矩阵所有元素都不为负值的条件，即X=WH。然而完全相等很难实现，所以我们只要求能够充分接近就可以了。这时需要一个代价函数（cost function）来表示两者的差距。一种代价函数是基于欧式距离的，一种是分离度的，两个都有对应的迭代公式。 介绍NMF算法的文章已经有很多了，本文主要介绍一下非负矩阵分解在分析癌症突变异质性中的作用。数据来自于nature论文 ——Mutational heterogeneity in cancer and the search for new cancer-associated genes ——的附表S3，是对2892个癌症病人的96中突变类型的统计。论文分析的是3083个癌症病人的数据，而附表S3只提供了突变个数在10个以上的数据，只有2892个病人，因此不求能够得到和论文完全一致的结果。现在的数据是每个癌症样本在96种突变类型上的分布，通过非负矩阵分解，我们期望得到每个样本在不同的突变谱上的分布，所谓突变谱就是把几个突变类型综合在一起的形式，例如*CpG岛处的突变代表了一些C参与的突变类型。下图为论文中找到的6个突变谱，包含了目前已知的突变过程。 数据归一化：某个样本的某种类型的突变可能是由于这种类型的突变的测序覆盖度比较好导致的，因此需要对每种突变类型进行归一化处理。对每个样本s和突变类型c，定义\\({n_{cs}}\\)为观察到的突变数，\\({N_{cs}}\\)为具有足够覆盖度碱基数，则样本总的突变频率为$\\mu _s=\\sum _c{n_{cs}} / \\sum _c{N_{cs}}$，每种突变的相对突变率$R_{cs}=(n_{cs}/N_{cs})/\\mu_s$，接着可以对$R_{cs}$矩阵进行非负矩阵分解。论文中并没有提供每个样本的每种突变类型的碱基数，只给了所有样本的平均值，所以这次计算用了这种并不合适的数据。R code:12345678910coverage &lt;- read.table('coverage.txt',header=F)[-1] mutation &lt;- read.csv('table_s3.csv',header=TRUE, row.names='name',stringsAsFactors=FALSE) mutation.spe &lt;- as.matrix(mutation[,2:97]) mutation.spectrum &lt;- matrix(0,nrow=2892,ncol=96,dimnames=dimnames(mutation.spe)) samlength=rep(0,2892) for (i in 1:nrow(mutation.spe))&#123; mius=sum(mutation.spe[i,])/sum(coverage) samlength[i]=mius*1000000 mutation.spectrum[i,]=as.matrix((mutation.spe[i,]/coverage)/mius) &#125;使用R包NMF进行矩阵分解，rank=6，nrun=50123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081library(NMF)spectrum.fit &lt;- nmf(mutation.spectrum, 6, nrun=50)spectrum.coef &lt;- coef(spectrum.fit)#spectrum.basis &lt;- t(spectrum.basis)basematrix &lt;- function(factor,base1,base2)&#123; base &lt;- rep(0,16) index=1 for (i in c('T','C','A','G'))&#123; for (j in c('T','C','A','G'))&#123; name=paste0(i,base1,j,'to',i,base2,j) base[index]=factor[name] index=index+1 &#125; &#125; return(matrix(base,nrow=4,byrow=T)) &#125;combinebase &lt;- function(index)&#123; CG = basematrix(spectrum.coef[index,],'C','G') CA = basematrix(spectrum.coef[index,],'C','A') CT = basematrix(spectrum.coef[index,],'C','T') AT = basematrix(spectrum.coef[index,],'A','T') AC = basematrix(spectrum.coef[index,],'A','C') AG = basematrix(spectrum.coef[index,],'A','G') return(cbind(rbind(CG,AT),rbind(CA,AC),rbind(CT,AG)))&#125;par(mfrow=c(2,3),mar=c(2,1,1,2))#plotfor (i in 1:6)&#123; data = combinebase(i) data=data/max(data) # generate 'empty' persp plot pmat = persp(x=c(0,10), y=c(0,10), z=matrix(c(0,.001,0,.001), nrow=2), xlim=c(0,10), ylim=c(0,10), zlim=c(0,1.5), theta=60, phi=25, d=5, box=F,border=NA) # define color ramp colorCG = matrix(rep('#FF0000',16),nrow=4) colorAT = matrix(rep('#7F4CB2',16),nrow=4) colorCA = matrix(rep('#00B2B2',16),nrow=4) colorAC = matrix(rep('#0033CC',16),nrow=4) colorCT = matrix(rep('#FFFF00',16),nrow=4) colorAG = matrix(rep('#19CC19',16),nrow=4) colorM = cbind(rbind(colorCG,colorAT),rbind(colorCA,colorAC),rbind(colorCT,colorAG)) # draw each bar: from left to right ... for (i in 1:nrow(data))&#123; # ... and back to front for (j in ncol(data):1)&#123; xy = which(data == data[i,j], arr.ind=TRUE) # side facing y x = rep(xy[1]-0.1,4) y = c(xy[2]-0.9,xy[2]-0.1,xy[2]-0.1,xy[2]-0.9) z = c(0,0,data[i,j],data[i,j]) polygon(trans3d(x, y, z, pmat), col=colorM[i,j], border=1) # side facing x x = c(xy[1]-0.9,xy[1]-0.1,xy[1]-0.1,xy[1]-0.9) y = rep(xy[2]-0.9,4) z = c(0,0,data[i,j],data[i,j]) polygon(trans3d(x, y, z, pmat), col=colorM[i,j], border=1) # top side x = c(xy[1]-0.9,xy[1]-0.1,xy[1]-0.1,xy[1]-0.9) y = c(xy[2]-0.9,xy[2]-0.9,xy[2]-0.1,xy[2]-0.1) z = rep(data[i,j],4) polygon(trans3d(x, y, z, pmat), col=colorM[i,j], border=1) &#125; &#125;&#125; 从上图可以看出算法找到的突变谱分别为Tp*C-&gt;mut, Tp*A-&gt;T, C-&gt;A, misc, misc, *CpG-&gt;T。最后查看每种类型的癌症在这六种突变谱上的分布，即确认是否有的癌症只具有一种突变谱还是多种，可以通过绘制热图来查看。这里使用论文的可视化方法，通过绘制径向图查看。R绘图包plotrix中的radial.plot函数需要提供两个参数，即径向的长度和角度（以弧度为单位）。这里用每个样本的总突变率为长度。对于每个样本s，使$i_{sr}$表示在六个突变谱中第r-th个最大的权重，角度可以通过这个公式计算$\\alpha_s=2\\pi\\sum _{r=0}^K{i_r(1/K)^r}$。第一个最大的权重决定样本在整个圆周的哪个扇区，第二大权重决定在上个扇区中的小扇区，以此推导。R code:123456789101112131415161718192021222324252627282930313233343536spectrum.sample &lt;- basis(spectrum.fit)samlength=rep(0,2892)radial=rep(0,2892)for (i in 1:2892)&#123; ir=order(spectrum.sample[i,1:6],decreasing = T) radial[i]=2*pi*sum(ir*(1/6)**(1:6))&#125;colorlist &lt;- list('AML'='#12A6DA','Bladder'='#E5E515','Breast'='#F37F81','CLL'='#A94399','Colorectal'='#0F9B5A','Carcinoid'='#6F2011','Cervical'='#F58020','DLBCL'='#2C2C81','Oesophageal adenocarcinoma'='#5DBB46','Ewing sarcoma'='#664182','Glioblastoma multiforme'='#8B715D','Head and neck'='#364EA1','Kidney clear cell'='#543C1C','Kidney papillary cell'='#302636','Low-grade glioma'='#8D5967','Lung adenocarcinoma'='#B12124','Lung squamous cell carcinoma'='#ED1F24','Multiple myeloma'='#808233','Medulloblastoma'='#16304A','Melanoma'='#231F20','Neuroblastoma'='#39276B','Ovarian'='#CC80B4','Pancreas'='#543C1C','Prostate'='#B3B48C','Rhabdoid tumor'='#9999A6','Stomach'='#69BD45','Thyroid'='#A1A6D2')tumor_type=table(mutation[,1])collevel=rep('',27)for (i in 1:length(colorlist))&#123; collevel[i]=colorlist[[i]]&#125;colorpoint=rep('',2892)loc=1for (i in 1:27)&#123; #tumor=names(tumor_type[i]) colorpoint[loc:(loc+tumor_type[i]-1)]=rep(colorlist[[i]],tumor_type[i]) loc=loc+tumor_type[i]&#125;library(plotrix)radial.plot(log(samlength,10),radial,rp.type=\"s\",point.col=colorpoint,point.symbols=20, show.grid=T,grid.col=\"white\",show.grid.labels=0, radial.lim=c(log10(0.04),log10(100)), label.pos=c(pi/2,5*pi/6,7*pi/6,3*pi/2,11*pi/6,pi/6), labels=c('Tp*C-&gt;mut','Tp*A-&gt;T','C-&gt;A','misc','misc','*CpG-&gt;T'))radial.plot(rep(2,6),c(0,pi/3,pi*2/3,pi,pi*4/3,pi*5/3),rp.type='r', line.col = 'grey',lwd=2,add=T,radial.lim=c(log10(0.04),log10(100))) 下面绘制图例123plot(1:30,1:30,bty='n',type='n',xaxt='n',yaxt='n',xlab='',ylab='')legend(1,27,names(colorlist)[1:14],col=collevel[1:14],pch=19,pt.cex=1.5,bty='n')legend(18,27,names(colorlist)[15:27],col=collevel[15:27],pch=19,pt.cex=1.5,bty='n') 从图中可以看到一些有意义的结果，例如AML主要分布在Tp*A-&gt;T的突变，而肺癌主要是C-&gt;A的突变。然后由于数据不完整，归一化做的比较，最终得到的结果也不是太好，并没有很好地解释癌症突变谱异质性。","categories":[],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://mtpgm.com/tags/机器学习/"}]}]}